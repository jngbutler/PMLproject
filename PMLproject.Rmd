---
title: "Practical Machine Learning Course Project"
author: "Jennifer Butler"
date: "October 7, 2015"
output: 
  html_document: 
    fig_caption: yes
    keep_md: yes
---

```{r knitr_options, echo=FALSE, message=FALSE, warning=FALSE}
library(e1071)
library(caret)
library(rpart)
library(MASS)
library(randomForest)
library(plot3D)
```

### Background
We can now collect a large amount of data about personal activity using devices that can be worn fairly unobtrusively on the body.  In this project, we examine data collected from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants as they were asked to perform barbell lifts in 5 different ways (1x correctly and 4x incorrectly).  Our goal is to predict how the exercise was performed (the "classe" variable) using machine learning algorithms.

### Loading, Partitioning, Cleaning, and Preprocessing Data

#### *Loading data*
The training data for this project are available at <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> and the test data containing the 20 test cases are available at <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>.

```{r cache=TRUE}
TrainingDataURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TestDataURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TrainingData <- read.csv(url(TrainingDataURL))
TestData <- read.csv(url(TestDataURL))
```

#### *Partitioning data*
First we partition our training data into **train** which we will use to train and fit our machine learning algorithm, and **trainTest** which we will use to validate our model and estimate out-of-sample error. Using a 60/40 partition, we end up with 11776 observations for the **train** sample and 7846 observations for the **trainTest** sample

```{r cache=TRUE}
set.seed(1234)
inTrain <- createDataPartition(y=TrainingData$classe,p=0.6,list=FALSE)
train <- TrainingData[inTrain,]       ## 11776 obs of 160 vars
trainTest <- TrainingData[-inTrain,]  ## 7846 obs of 160 vars
```

#### *Cleaning data*
The **train** dataset contains NA's, blanks, and #DIV/0! values.  We convert them all to NA's in order to quickly remove them.  There are 100 columns that have between 11540 to 11776 missing values (most of the observations are missing).  

```{r cache=TRUE}
train[train==""] <- NA
train[train=="#DIV/0!"] <- NA
table(colSums(is.na(train)))
```

After removing them, we are left with 60 columns that have all 11776 observations.

```{r cache=TRUE}
mTrain <- sapply(train,function(x) sum(is.na(x)))
train <- train[,-which(mTrain>0)] 
dim(train)
```

Next, we also remove columns 1 through 7 which are row indices, subject names, timestamps, and time windows.  These columns do not contain the relevant accelerometer information.  

```{r cache=TRUE}
names(train[,1:7])
```

After they are removed, we are left with 53 columns.

```{r}
train <- train[,-c(1:7)]
dim(train)
```

#### *Preprocessing by removing highly correlated variables*
With the exception of the **classe** variable (which is what we are trying to successfully predict), all of the other columns are either integers or numeric values.  In order to improve the performance of our algorithms, we now look for and remove any highly correlated (correlation coefficient above 0.9) variables in order to reduce multicollinearity. First we create a correlation matrix.  Then we use the **findCorrelation** function in the **caret** package to compare the absolute values of pair-wise correlations in the matrix - if two variables have a high correlation, it flags for removal the variable with the largest mean absolute correlation.  In our case it found 7 variables to be removed.    

```{r cache=TRUE}
trainVars <- train[,-53]
corrMatrix <- cor(trainVars[sapply(trainVars, is.numeric)])
ToRemove <- findCorrelation(corrMatrix,cutoff=.9)
names(trainVars[ToRemove])
train <- train[,-ToRemove]
```

Our final cleaned up and minimally preprocessed **train** data now consists of 11776 observations with one outcome variable **classe** and 45 candidate features to be used in training our models.  

```{r}
dim(train)
```

Below is the list of the columns in the data we will use to train our model.

```{r}
names(train)
```

### Building Machine Learning Algorithms
**Model 1: Quadratic discriminant analysis (qda)**
Because we have a categorical dependent variable and our predictors are all accelerometer information or calculations from such information (they are all integers or numeric rather than factors), we start with discriminant function analysis.  Linear discriminant analysis (LDA) is a classification method which attempts to find a linear combination of features that can separate two or more classes.  In our case, we will use quadratic discriminant analysis (QDA) which is a generalized version of LDA.  QDA does **not** require the assumption that the measurements for each class be normally distributed with equal variances, and it works well with decision boundaries that may not be linear.  Our **model1** yields an accuracy of 87.5%.

```{r cache=TRUE}
set.seed(1234)
model1 <- train(classe~.,data=train,method="qda")
model1
```

**Model 2: Recursive partitioning (rpart)**
We next turn to recursive partitioning (rpart) which attempts to create a decision tree by splitting a population into sub-populations an indefinite number of times until a particular stopping criterion is reached.  One criticism of **rpart** is that it does not work so well with continuous variables.  As you can see, the accuracy of the optimal tree from **model2** was only 51.5%!

```{r cache=TRUE}
set.seed(1234)
model2 <- train(classe~.,data=train,method="rpart")
model2
```

**Model 3: Random forest (randomForest)**
We then use random forest next because it is an ensemble learning method of classification that builds many trees using bootstrap random sample subsets of the data (rather than a single tree using the entire sample) for better performance. A good explanation of how random forests work can be found at <https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings>. 

```{r cache=TRUE}
set.seed(1234)
model3 <- randomForest(classe~.,data=train,importance=TRUE,proximity=TRUE)
model3
```

```{r echo=FALSE}
if (file.exists("model3.rda")) {load("model3.rda")} else {save(model3, file="model3.rda")}
```

The accuracy of this algorithm is 99.3%.  This is the algorithm we will choose to validate with the  **trainTest** sample that we partitioned out earlier.  

```{r}
sum(diag(model3$confusion))/sum(model3$confusion)
```

**Examining the machine learning algorithm using random forest**  
Let's take a closer look at our algorithm.  Below we can examine the features that the algorithm found to be important.  The "mean decrease accuracy" tells us how much accuracy is lost if we remove that predictor.  Therefore variables that have larger mean decreases in accuracy are more important.  The "mean decrease GINI" tells us the relative contribution of each predictor to the purity of the resulting nodes of the tree.  Therefore, variables that result in higher losses in purity (mean decrease GINI) are also considered more important to the algorithm.

```{r cache=TRUE}
varImpPlot(model3)
```

Below we display a 3-D plot of three of the most important predictors: "yaw_belt", "pitch_belt", and "magnet_dumbbell_z".

```{r cache=TRUE, warning=FALSE}
scatter3D(x=train$pitch_belt,y=train$yaw_belt,z=train$magnet_dumbbell_z,
          col=train$classe,colkey=FALSE,
          xlab="pitch_belt",ylab="yaw_belt",zlab="magnet_dumbbell_z")
```

### Cross-Validation and Out-of-Sample Error
The **trainTest** sample was a 40% partition from the original **TrainingData** and it contains 7846 observations of 160 variables.  The **trainTest** sample is completely untouched - we have not done any cleaning or preprocessing on this partition.  We will use this partition as our cross-validation to test **model3**, our machine learning algorithm which we trained using **randomForest**, and estimate our out-of-sample error.  

```{r}
predictions <- predict(model3,newdata = trainTest)
confusionMatrix(predictions,trainTest$classe)
```

We can see above that the **accuracy** of our algorithm on **trainTest** is **0.9929** (nearly identical to the accuracy of the data we trained on which was 0.9932033) with a 95% confidence interval between 0.9907 and 0.9946.  In other words, we can be 95% confident that our accuracy for correctly predicting **classe** is between 99.1% and 99.5%.

The OOB (out-of-bag) estimate of error rate on the data we trained on was 0.68% or 0.0068 (see summary of **model3** above).  The **randomForest** function does cross-validation internally as part of its run by using bootstrap random samples where about one-third of the cases are left out ("out-of-bag") for each of 500 trees that it builds (using the function's default number of trees).  Therefore we expect the OOB error rate of 0.0068 to be close to the actual out-of-sample error rate on the **trainTest** data.  Looking at the confusion matrix for our predicted values, it turns out that the OOB estimate error rate is indeed very close to our actual **out-of-sample error** of 1-accuracy, or 1-0.9929, which is **0.0071**.

### Application of the Final Model to the 20 Test Cases
Finally, we will apply our machine learning algorithm to the 20 test cases supplied in the **TestData** sample.  Based on the performance of our algorithm on both **train** (the data we trained on) as well as **trainTest** (the validation data we tested our algorithm on), we expect to be able to correctly predict upwards of 99% of the test cases.

```{r cache=TRUE}
predictionsTestCases <- predict(model3,newdata = TestData)
predictionsTestCases
```

We follow the instructions to create 20 text files each with a single capital letter and submit each file individually on the course project submission website.  We get a "You are correct!" feedback for each submission and find that we have successfully classified all 20 test cases.
